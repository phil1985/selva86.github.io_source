<blockquote>
<p>This chapter explains the purpose of some of the most commonly used statistical tests and how to implement them in R</p>
</blockquote>
<h2>1. Shapiro Test</h2>
<h4>Why is it used?</h4>
<p>To test if a sample follows a <em>normal distribution</em>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">shapiro.test</span>(numericVector) <span class="co"># Does myVec follow a normal disbn?</span></code></pre></div>
<p>Lets see how to do the test on a sample from a normal distribution.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Example: Test a normal distribution</span>
normaly_disb &lt;-<span class="st"> </span><span class="kw">rnorm</span>(<span class="dv">100</span>, <span class="dt">mean=</span><span class="dv">5</span>, <span class="dt">sd=</span><span class="dv">1</span>) <span class="co"># generate a normal distribution</span>
<span class="kw">shapiro.test</span>(normaly_disb)  <span class="co"># the shapiro test.</span>
<span class="co">#&gt; </span>
<span class="co">#&gt;  Shapiro-Wilk normality test</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; data:  normaly_disb</span>
<span class="co">#&gt; W = 0.98814, p-value = 0.5184</span></code></pre></div>
<h4>How to interpret?</h4>
<p>The null hypothesis here is that the sample being tested is normally distributed. Since the p Value is not less that the significane level of 0.05, we don’t reject the null hypothesis. Therefore, the tested sample is confirmed to follow a normal distribution (thou, we already know that!).</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Example: Test a uniform distribution</span>
not_normaly_disb &lt;-<span class="st"> </span><span class="kw">runif</span>(<span class="dv">100</span>)  <span class="co"># uniform distribution.</span>
<span class="kw">shapiro.test</span>(not_normaly_disb)
<span class="co">#&gt; </span>
<span class="co">#&gt;  Shapiro-Wilk normality test</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; data:  not_normaly_disb</span>
<span class="co">#&gt; W = 0.94631, p-value = 0.0004781</span></code></pre></div>
<h4>How to interpret?</h4>
<p>If p-Value is less than the significance level of 0.05, the null-hypothesis that it is normally distributed can be rejected, which is the case here.</p>
<h2>2. One Sample t-Test</h2>
<h4>Why is it used?</h4>
<p>It is a parametric test used to test if the mean of a sample from a normal distribution could reasonably be a specific value.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">x &lt;-<span class="st"> </span><span class="kw">rnorm</span>(<span class="dv">50</span>, <span class="dt">mean =</span> <span class="dv">10</span>, <span class="dt">sd =</span> <span class="fl">0.5</span>)
<span class="kw">t.test</span>(x, <span class="dt">mu=</span><span class="dv">10</span>) <span class="co"># testing if mean of x could be</span>
<span class="co">#&gt; </span>
<span class="co">#&gt;  One Sample t-test</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; data:  x</span>
<span class="co">#&gt; t = 0.67716, df = 49, p-value = 0.5015</span>
<span class="co">#&gt; alternative hypothesis: true mean is not equal to 10</span>
<span class="co">#&gt; 95 percent confidence interval:</span>
<span class="co">#&gt;   9.912529 10.176380</span>
<span class="co">#&gt; sample estimates:</span>
<span class="co">#&gt; mean of x </span>
<span class="co">#&gt;  10.04445</span></code></pre></div>
<h4>How to interpret?</h4>
<p>In above case, the p-Value is not less than significance level of 0.05, therefore the null hypothesis that the mean=10 cannot be rejected. Also note that the 95% confidence interval range includes the value 10 within its range. So, it is ok to say the mean of ‘x’ is 10, especially since ‘x’ is assumed to be normally distributed. In case, a normal distribution is not assumed, use wilcoxon signed rank test shown in next section.</p>
<p>Note: Use conf.level argument to adjust the confidence level.</p>
<h2>3. Wilcoxon Signed Rank Test</h2>
<p>Why / When is it used? Testing the mean of a sample when normal distribution is not assumed. Wilcoxon signed rank test can be an alternative to t-Test, especially when the data sample is not assumed to follow a normal distribution. It is a non-parametric method used to test if an estimate is different from its true value.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">wilcox.test</span>(input.vector, <span class="dt">mu =</span> m, <span class="dt">conf.int =</span> <span class="ot">TRUE</span>)</code></pre></div>
<h4>How to interpret?</h4>
<p>If p-Value &lt; 0.05, reject the null hypothesis and accept the alternate mentioned in your R code’s output. Type example(wilcox.test) in R console for illustration.</p>
<h2>4. Two Sample t-Test and Wilcoxon Rank Sum Test</h2>
<p>Both t.Test and Wilcoxon rank test can be used to compare the mean of 2 samples. The difference is t-Test assumes the samples being tests is drawn from a normal distribution, while, Wilcoxon’s rank sum test does not.</p>
<h4>How to implement in R?</h4>
<p>Pass the two numeric vector samples into the t.test() when sample is distributed ‘normal’y and wilcox.test() when it isn’t assumed to follow a normal distribution.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">x &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="fl">0.80</span>, <span class="fl">0.83</span>, <span class="fl">1.89</span>, <span class="fl">1.04</span>, <span class="fl">1.45</span>, <span class="fl">1.38</span>, <span class="fl">1.91</span>, <span class="fl">1.64</span>, <span class="fl">0.73</span>, <span class="fl">1.46</span>)
y &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="fl">1.15</span>, <span class="fl">0.88</span>, <span class="fl">0.90</span>, <span class="fl">0.74</span>, <span class="fl">1.21</span>)
<span class="kw">wilcox.test</span>(x, y, <span class="dt">alternative =</span> <span class="st">&quot;g&quot;</span>)  <span class="co"># g for greater</span>
<span class="co">#&gt; </span>
<span class="co">#&gt;  Wilcoxon rank sum test</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; data:  x and y</span>
<span class="co">#&gt; W = 35, p-value = 0.1272</span>
<span class="co">#&gt; alternative hypothesis: true location shift is greater than 0</span></code></pre></div>
<p>With a p-Value of 0.1262, we cannot reject the null hypothesis that both x and y have same means.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">t.test</span>(<span class="dv">1</span>:<span class="dv">10</span>, <span class="dt">y =</span> <span class="kw">c</span>(<span class="dv">7</span>:<span class="dv">20</span>))      <span class="co"># P = .00001855</span>
<span class="co">#&gt; </span>
<span class="co">#&gt;  Welch Two Sample t-test</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; data:  1:10 and c(7:20)</span>
<span class="co">#&gt; t = -5.4349, df = 21.982, p-value = 1.855e-05</span>
<span class="co">#&gt; alternative hypothesis: true difference in means is not equal to 0</span>
<span class="co">#&gt; 95 percent confidence interval:</span>
<span class="co">#&gt;  -11.052802  -4.947198</span>
<span class="co">#&gt; sample estimates:</span>
<span class="co">#&gt; mean of x mean of y </span>
<span class="co">#&gt;       5.5      13.5</span></code></pre></div>
<p>With p-Value &lt; 0.05, we can safely reject the null hypothesis that there is no difference in mean.</p>
<h4>What if we want to do a 1-to-1 comparison of means for values of x and y?</h4>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Use paired = TRUE for 1-to-1 comparison of observations.</span>
<span class="kw">t.test</span>(x, y, <span class="dt">paired =</span> <span class="ot">TRUE</span>) <span class="co"># when observations are paired, use &#39;paired&#39; argument.</span>
<span class="kw">wilcox.test</span>(x, y, <span class="dt">paired =</span> <span class="ot">TRUE</span>) <span class="co"># both x and y are assumed to have similar shapes</span></code></pre></div>
<h4>When can I conclude if the mean’s are different?</h4>
<p>Conventionally, If the p-Value is less than significance level (ideally 0.05), reject the null hypothesis that both means are the are equal.</p>
<h2>5. Kolmogorov And Smirnov Test</h2>
<p>Kolmogorov-Smirnov test is used to check whether 2 samples follow the same distribution.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">ks.test</span>(x, y) <span class="co"># x and y are two numeric vector</span></code></pre></div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># From different distributions</span>
x &lt;-<span class="st"> </span><span class="kw">rnorm</span>(<span class="dv">50</span>)
y &lt;-<span class="st"> </span><span class="kw">runif</span>(<span class="dv">50</span>)
<span class="kw">ks.test</span>(x, y)  <span class="co"># perform ks test</span>
<span class="co">#&gt; </span>
<span class="co">#&gt;  Two-sample Kolmogorov-Smirnov test</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; data:  x and y</span>
<span class="co">#&gt; D = 0.44, p-value = 9.909e-05</span>
<span class="co">#&gt; alternative hypothesis: two-sided</span></code></pre></div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Both from normal distribution</span>
x &lt;-<span class="st"> </span><span class="kw">rnorm</span>(<span class="dv">50</span>)
y &lt;-<span class="st"> </span><span class="kw">rnorm</span>(<span class="dv">50</span>)
<span class="kw">ks.test</span>(x, y)  <span class="co"># perform ks test</span>
<span class="co">#&gt; </span>
<span class="co">#&gt;  Two-sample Kolmogorov-Smirnov test</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; data:  x and y</span>
<span class="co">#&gt; D = 0.14, p-value = 0.7166</span>
<span class="co">#&gt; alternative hypothesis: two-sided</span></code></pre></div>
<h4>How to tell if they are from the same distribution ?</h4>
<p>If p-Value &lt; 0.05 (significance level), we reject the null hypothesis that they are drawn from same distribution. In other words, p &lt; 0.05 implies x and y from different distributions</p>
<h2>6. Fisher’s F-Test</h2>
<p>Fisher’s F test can be used to check if two samples have same variance.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">var.test</span>(x, y)  <span class="co"># Do x and y have the same variance?</span></code></pre></div>
<p>Alternatively fligner.test() and bartlett.test() can be used for the same purpose.</p>
<h2>7. Chi Squared Test</h2>
<p>Chi-squared test in R can be used to test if two categorical variables are dependent, by means of a contingency table.</p>
<p>Example use case: You may want to figure out if big budget films become box-office hits. We got 2 categorical variables (Budget of film, Success Status) each with 2 factors (Big/Low budget and Hit/Flop), which forms a 2 x 2 matrix.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">chisq.test</span>(matrix, <span class="dt">correct =</span> <span class="ot">FALSE</span>)  <span class="co"># Yates continuity correction not applied</span>
<span class="co">#or</span>
<span class="kw">summary</span>(<span class="kw">table</span>(x, y)) <span class="co"># performs a chi-squared test.</span></code></pre></div>
<p>Pearson’s Chi-squared test data: M X-squared = 30.0701, df = 2, p-value = 2.954e-07</p>
<h4>How to tell if x, y are independent?</h4>
<p>There are two ways to tell if they are independent:</p>
<ol style="list-style-type: decimal">
<li><p><strong>By looking at the p-Value</strong>: If the p-Value is less that 0.05, we fail to reject the null hypothesis that the x and y are independent. So for the example output above, (p-Value=2.954e-07), we reject the null hypothesis and conclude that x and y are not independent.</p></li>
<li><p><strong>From Chi.sq value</strong>: For 2 x 2 contingency tables with 2 degrees of freedom (d.o.f), if the Chi-Squared calculated is greater than 3.841 (critical value), we reject the null hypothesis that the variables are independent. To find the critical value of larger d.o.f contingency tables, use qchisq(0.95, n-1), where n is the number of variables.</p></li>
</ol>
<h2>8. Correlation</h2>
<h4>Why is it used?</h4>
<p>To test the linear relationship of two continuous variables</p>
<p>The cor.test() function computes the correlation between two continuous variables and test if the y is dependent on the x. The null hypothesis is that the true correlation between <em>x</em> and <em>y</em> is zero.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">cor.test</span>(x, y) <span class="co"># where x and y are numeric vectors.</span></code></pre></div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">cor.test</span>(cars$speed, cars$dist)
<span class="co">#&gt; </span>
<span class="co">#&gt;  Pearson&#39;s product-moment correlation</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; data:  cars$speed and cars$dist</span>
<span class="co">#&gt; t = 9.464, df = 48, p-value = 1.49e-12</span>
<span class="co">#&gt; alternative hypothesis: true correlation is not equal to 0</span>
<span class="co">#&gt; 95 percent confidence interval:</span>
<span class="co">#&gt;  0.6816422 0.8862036</span>
<span class="co">#&gt; sample estimates:</span>
<span class="co">#&gt;       cor </span>
<span class="co">#&gt; 0.8068949</span></code></pre></div>
<h4>How to interpret?</h4>
<p>If the p Value is less than 0.05, we reject the null hypothesis that the true correlation is zero (i.e. they are independent). So in this case, we reject the null hypothesis and conclude that <em>dist</em> is dependent on <em>speed</em>.</p>
<h2>9. More Commonly Used Tests</h2>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">fisher.test</span>(contingencyMatrix, <span class="dt">alternative =</span> <span class="st">&quot;greater&quot;</span>)  <span class="co"># Fisher&#39;s exact test to test independence of rows and columns in contingency table</span>
<span class="kw">friedman.test</span>()  <span class="co"># Friedman&#39;s rank sum non-parametric test </span></code></pre></div>
<p>There are more useful tests available in various other packages.</p>
<p>The package <em>lawstat</em> has a good collection. The outliers package has a number of test for testing for presence of outliers.</p>
